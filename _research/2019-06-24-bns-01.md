---
layout: post
title:  "강화학습을 이용한 Blade & Soul 비무 AI 제작기"
author: inseok
categories: [ Research ]
image: assets/images/BNS1_2.jpg
featured: false
---
# 강화학습을 이용한 Blade & Soul 비무 AI 제작기

## 1. 시작하며
2018년 9월 15일, 강화학습팀에서 만든 B&S 비무 AI가 B&S World Championship Blind Match에서 프로게이머를 상대로 승리를 따냈습니다.

![BNS World Championship 현장](/assets/images/BNS1_1.jpg) 
#####  B&S World Championship 현장. 약 5,000명의 관중이 행사에 참석하였다 (사진 교체)

B&S 비무와 같은 복잡한 3D fighting 게임 분야에서 neural network 기반 AI가 프로게이머를 꺾은 것은 저희가 알기로는 처음 있는 일인데요1. 이 문제를 풀면서 겪었던 연구 경험이 다른 강화학습 연구자분들에게도 도움이 되었으면 하는 마음에 글을 쓰게 되었습니다. 따라서 이 글에서는 B&S 비무 콘텐츠에 강화학습을 어떻게 적용하였는지, 주로 기술적인 부분에 초점을 맞춰 서술해 나갈 것입니다.

## 2. B&S 게임을 강화학습으로 풀기 어려운 이유
B&S 비무는 게임 AI 연구 주제 분류 중 fighting 게임에 속한다고 볼 수 있습니다. 바둑이나 체스와 같은 턴제 게임은 상대의 action이 수행되고 난 후 내 action을 선택하지만, fighting game에서는 이 과정이 동시에 일어납니다. 따라서 B&S에서는 상대방의 action이 무엇인지 모른 채 내 action을 선택해야 하며, 이는 최적 policy 학습을 어렵게 만드는 요소 중 하나입니다.

(TODO:사진 넣기)
![Fighting Game Problem](/assets/images/BNS1_2.jpg)
##### Fighting 게임 특성 때문에 B&S에서 발생하는 문제2
두 번째로 문제의 space를 꼽을 수 있습니다. B&S 캐릭터들은 최소 45가지 이상의 skill과 9가지의 move(8방향 + no move) 중 각각 하나를 선택해야 하며 target도 결정해야 합니다. 또한 Input은 100ms마다 들어오며, 1초당 최대 4번의 action을 수행할 수 있고, 비무 한 게임의 길이는 최대 180초입니다. Target을 제외한 가능한 action의 평균 값을 이용하여 문제의 space를 계산해보면, space = 8(skill) * 9(direction) ^ 1200(game length)입니다. 실제로 사용되는 공간은 이보다 작을지라도 문제 공간이 일반적인 보드게임보다는 훨씬 크다는 사실을 알 수 있습니다.

B&S 역사 직업의 skill 개수. skill 정보가 세부적으로 바뀌는 경우도 있다

세 번째로 어려운 점은 B&S 게임이 실시간 게임이라는 점입니다. 턴제 게임과는 달리 B&S에서는 AI가 100ms 내에 decision을 내려야 하므로 모델의 크기를 제한적으로 설계할 수밖에 없습니다. 또한 “AlphaGo”에서 사용했던 MCTS와 같은 search 방법을 사용할 수 없어 큰 공간을 커버하기 어렵습니다.

“AlphaGo”에서 사용한 MCTS5. 하지만 실시간 게임에서는 사용하기가 어렵다

그 밖에도 다음과 같은 문제가 존재합니다.

- PvP 게임인 Fighting 게임의 특성상 AI의 적절한 대전 상대가 없다는 점
- human 플레이어 데이터 부족으로 인해 supervised learning에 한계가 있는 점
- AI 테스트용 게임이 아닌 실제 라이브에서 서비스되는 게임이기 때문에 발생하는 여러 환경적 이슈

## 3. 설계 과정
강화학습 AI를 만들기 위해서는 알고리즘, 네트워크, 학습 방법 및 시스템 등의 설계가 필요합니다. 각 해당 파트에서 저희가 고려했던 점, 제약 사항 등에 대해 서술하겠습니다.

### 3.1 알고리즘
강화학습 연구자들이 여러 알고리즘을 끊임없이 발표하고 있는데요. 저희는 이 중에서 성능이 검증된 Q-learning 기반 알고리즘과 Policy Gradient 알고리즘, 그리고 Actor Critic 알고리즘을 위주로 조사하였습니다.
최종적으로 선정된 알고리즘은 ACER6이며, 그 이유는 다음 조건을 모두 충족하였기 때문입니다.
- Stochastic Policy
- Discrete Action Space
- Off-policy
그렇다면, 이러한 3가지 조건을 만족시키는 알고리즘을 선정해야 했던 이유에 대해 알아보도록 하겠습니다.

2018년 초, 저희가 선택을 고려하고 있던 알고리즘으로는 Q-learning 기반 방법론인 DQN6, Double DQN7, Dueling DDQN8, Prioritized DDQN9, 그리고 Policy Gradient 방법론인 DDPG10와 PPO11, 마지막으로 Actor-critic 방법론인 A3C12와 ACER 등이 있었습니다.
이 중 DQN 라인의 알고리즘은 모두 deterministic policy에 속하고, DDPG는 continuous action space에서 동작하는 알고리즘이며 PPO와 A3C는 on-policy 알고리즘입니다. 아래 표에서 볼 수 있듯이 위의 3가지 조건을 모두 만족하는 알고리즘은 ACER 뿐입니다.

#### 3가지 특성에 따른 알고리즘 비교
|                       	| DQN 	| DDPG 	| PPO 	| A3C 	| ACER 	|
|-----------------------	|:---:	|:----:	|:---:	|:---:	|:----:	|
| Stochastic Policy     	|  X  	|   O  	|  O  	|  O  	|   O  	|
| Discrete Action Space 	|  O  	|   X  	|  O  	|  O  	|   O  	|
| Off Policy            	|  O  	|   O  	|  X  	|  X  	|   O  	|

먼저, Value-based deterministic policy인 DQN과 DDQN 그리고 prioritized DDQN은 하나의 고정된 상대에 대해서는 승률이 95%에 도달하였지만 두 가지 이상의 agent를 상대로 학습했을 때에는 agent가 하나일 때보다 성능이 떨어지는 모습을 보였습니다. (여기서 상대는 B&S 무한의 탑 기반 1.0version AI와 human data를 이용한 supervised learning AI를 사용하였습니다. 향후 각각 1.0과 SL이라고 칭하겠습니다)

![Prioritized DDQN Learning Curve](/assets/images/BNS1_3.jpg)
##### Prioritized DDQN의 학습 곡선 (좌: 승률, 우: 체력 차이). 1.0과 SL의 두 가지 상대에 대해서는 승률이 70%에 수렴하는 현상을 보임

위 그래프와 같이 두 가지 이상의 agent를 상대로 학습했을 때 승률이 떨어지는 이유는 AI의 policy가 deterministic하다는 특성에서 기인한 것으로 보입니다. 앞서 말씀드렸듯이 fighting 게임에서는 상대가 무슨 action을 선택했는지 그 action이 발동하기 전까지는 알 수 없습니다. 만약 상대가 하나로 고정되어 있다면 PvE인 Atari game처럼 환경으로 여기고 학습하면 됩니다. 그렇지만 상대가 둘 이상으로 늘어나면, 같은 state에서 나올 수 있는 action이 다양해져 deterministic policy로는 학습하기가 힘들어집니다.

다음으로는 on-policy 알고리즘인 A3C를 이용해 학습을 진행해 보았습니다. 게임 서버의 구조로 인해 중도에 게임을 멈출 수 없어 매 경기가 끝난 후 축적된 log 파일을 이용하여 학습하였습니다. 상대가 1.0 하나일 때에는 학습이 잘 되었지만, 1.0과 SL 두 가지 agent를 상대로 학습하자 승률이 지나치게 더디게 증가하였습니다. 이에 따라 simulation 수를 4배로 늘려 학습을 진행하니 승률이 점점 하락하였습니다. 분석 결과, log 파일을 이용한 학습으로 인해 발생한 policy lag(learner policy와 behavior policy의 차이가 커지는 것) 현상 때문이었습니다. 2018년 2월에 DeepMind에서 발표한 IMPALA14 논문에 policy-lag 현상에 대한 자세한 실험 결과가 있으니 참고하시면 좋을 것 같습니다. 따라서 저희 게임 환경에서는 on-policy 알고리즘을 사용하기 힘들다는 결론을 얻었습니다.

![A3C Learning Results](/assets/images/BNS1_4.jpg)
##### 1.0과 SL을 상대로 한 A3C 학습한 결과 (DQN과는 그래프 x축의 step count 방식이 설계됨)

![4X simulation](/assets/images/BNS1_5.jpg)
##### Simulation을 4배로 늘려 학습한 결과. 위의 그래프에 이어 학습을 진행했지만 평가 승률은 계속 하락하고 있다

이와 같은 이유로 여러 알고리즘 후보 중 discrete action space에서 동작하고, off-policy이면서 stochastic policy인 ACER를 최종적으로 선정하였습니다. ACER로 학습한 결과 DQN과 A3C보다 초, 중반부 학습 속도도 더 빠를 뿐만 아니라 최종 성능도 두 가지 학습 상대에 대해 100%에 근접한 승률을 달성하였습니다.

![ACER Learning Results](/assets/images/BNS1_6.jpg)
##### 1.0과 SL을 상대로 한 ACER 학습 결과. 시간상으로는 약 30분만에 90%를 돌파하였으며 최종적으로 승률 100%에 근접한 모습 (좌: 체력 차이, 우: 승률)

이것으로 1부 마지막인 알고리즘 선정까지 살펴보았습니다. 2부에서는 네트워크 설계와 학습 방법론(고성능 AI를 만들기 위한 이슈)에 대해 살펴보도록 하겠습니다.

## 3.2는 다음 글로 돌아오겠습니다 :)

## 참고 문헌
- Creating Pro-Level AI for Real-Time Fighting Game with Deep Reinforcement Learning (B&S)
- https://kensaku.tistory.com/1711 (가위바위보)
- Mastering the game of Go with deep neural networks and tree search (AlphaGo)
- Sample Efficient Actor-Critic with Experience Replay (ACER)
- Playing Atari with Deep Reinforcement Learning (DQN)
- Deep Reinforcement Learning with Double Q-learning (DDQN)
- Dueling Network Architectures for Deep Reinforcement Learning (Dueling DDQN)
- Prioritized Experience Replay (Prioritized DDQN)
- Continuous Control with Deep Reinforcement Learning (DDPG)
- Proximal Policy Optimization Algorithms (PPO)
- Asynchronous Methods for Deep Reinforcement Learning (A3C)
- IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures
